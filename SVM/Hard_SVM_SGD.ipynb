{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Soft_SVM_SGD.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1UqBtAvhEHD"
      },
      "source": [
        "# Soft SVM (Support vector machine) using Stochastic Gradient Descent\n",
        "# min(w) (||w||^2 + hinge_loss(w)) \n",
        "# where hinge_loss((w, b),(x, y)) = 1/m(summation_over_m(max{0, 1 − y(hw, xi + b)}))\n",
        "\n",
        "\n",
        "## Step 1: Import All required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bDvNdYa9CjLj"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwbCo4pnhNcv"
      },
      "source": [
        "## Step 2: Read CSV file (For this i have uploded the file on google colab) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKpaEb3HDOUT",
        "outputId": "4e09cea0-9ac5-4743-95a3-6900e3615328"
      },
      "source": [
        "df = pd.read_csv('file.csv')\n",
        "print(df.shape,df)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2665, 6)       Temperature   Humidity       Light          CO2  HumidityRatio  Occupancy\n",
            "0       23.700000  26.272000  585.200000   749.200000       0.004764          1\n",
            "1       23.718000  26.290000  578.400000   760.400000       0.004773          1\n",
            "2       23.730000  26.230000  572.666667   769.666667       0.004765          1\n",
            "3       23.722500  26.125000  493.750000   774.750000       0.004744          1\n",
            "4       23.754000  26.200000  488.600000   779.000000       0.004767          1\n",
            "...           ...        ...         ...          ...            ...        ...\n",
            "2660    24.290000  25.700000  808.000000  1150.250000       0.004829          1\n",
            "2661    24.330000  25.736000  809.800000  1129.200000       0.004848          1\n",
            "2662    24.330000  25.700000  817.000000  1125.800000       0.004841          1\n",
            "2663    24.356667  25.700000  813.000000  1123.000000       0.004849          1\n",
            "2664    24.408333  25.681667  798.000000  1124.000000       0.004860          1\n",
            "\n",
            "[2665 rows x 6 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pqvpAIihVza"
      },
      "source": [
        "## Step 3: Store the features and target in X , Y seperately and since target has value 0,1 change label 0 to -1 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poQAQWxkDlIQ",
        "outputId": "93485263-47c0-4069-ba86-7087133d22fd"
      },
      "source": [
        "X = df.iloc[:,:-1].values\n",
        "Y = df.iloc[:,-1:].values\n",
        "print(X.shape,X)\n",
        "Y[Y==0]=-1\n",
        "print(Y.shape,Y)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2665, 5) [[2.37000000e+01 2.62720000e+01 5.85200000e+02 7.49200000e+02\n",
            "  4.76416302e-03]\n",
            " [2.37180000e+01 2.62900000e+01 5.78400000e+02 7.60400000e+02\n",
            "  4.77266099e-03]\n",
            " [2.37300000e+01 2.62300000e+01 5.72666667e+02 7.69666667e+02\n",
            "  4.76515255e-03]\n",
            " ...\n",
            " [2.43300000e+01 2.57000000e+01 8.17000000e+02 1.12580000e+03\n",
            "  4.84075873e-03]\n",
            " [2.43566667e+01 2.57000000e+01 8.13000000e+02 1.12300000e+03\n",
            "  4.84855928e-03]\n",
            " [2.44083333e+01 2.56816667e+01 7.98000000e+02 1.12400000e+03\n",
            "  4.86020770e-03]]\n",
            "(2665, 1) [[1]\n",
            " [1]\n",
            " [1]\n",
            " ...\n",
            " [1]\n",
            " [1]\n",
            " [1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjSE8T5ihdNM"
      },
      "source": [
        "## Step 4: Processing the Data and Splitting\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZfy-oNAEACo",
        "outputId": "4ee22e6d-ffd2-4fc2-bec4-b25c6c9a1592"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "min_max_scaler = preprocessing.MinMaxScaler()\n",
        "X_scale = min_max_scaler.fit_transform(X)\n",
        "X_scale"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.83168317, 0.44513204, 0.34479305, 0.33003334, 0.70421197],\n",
              "       [0.8359604 , 0.44705255, 0.34078657, 0.34152347, 0.70830847],\n",
              "       [0.83881188, 0.44065084, 0.33740855, 0.35103018, 0.70468898],\n",
              "       ...,\n",
              "       [0.98138614, 0.38410243, 0.48136692, 0.71638882, 0.74113545],\n",
              "       [0.98772277, 0.38410243, 0.47901016, 0.71351629, 0.74489576],\n",
              "       [1.        , 0.38214635, 0.47017234, 0.71454219, 0.75051096]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWkz0kv_mRqT"
      },
      "source": [
        "A. Split the data in 70,30 for training and testing the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8yozto7mOKz",
        "outputId": "8a9a00b6-b77c-49ef-a2ee-c169e7fa900e"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X_scale, Y, test_size=0.3, random_state=1)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1865, 5) (800, 5) (1865, 1) (800, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgkjzYQmheUB"
      },
      "source": [
        "B. Split the data in 80,20 for training and testing the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miV86CSwhjt3",
        "outputId": "9eeb9a6c-edbc-4925-b428-c94d7c150018"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2132, 5) (533, 5) (2132, 1) (533, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOQq3SrIiBVo"
      },
      "source": [
        "C. Split the data in 90,10 for training and testing the model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDNx6FeziGfn",
        "outputId": "20bc5de0-6571-48aa-f676-3a6088569e50"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=1)\n",
        "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2398, 5) (267, 5) (2398, 1) (267, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgyzBmnHiJKA"
      },
      "source": [
        "## Step 5: Defining class for optimizing the weight vector w using the stochastic gradient descent.\n",
        "### In stochastic gradient descent algorithm we minimise the true risk by creating a function that calculates the hinge loss for examples and then we update w using the sub gradient.\n",
        "### As hinge loss is given by :\n",
        "## hinge_loss((w, b),(x, y)) = max{0, 1 − y(hw, xi + b)}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGifnzNfEFEA"
      },
      "source": [
        "# Defining the class my SVM.\n",
        "class mySVM:\n",
        "  # Defining the constructor and assigning the values C,b,W\n",
        "  def __init__(self, C=1.0):\n",
        "    self.C = C\n",
        "    self.W = 0\n",
        "    self.b = 0\n",
        "\n",
        "  # Defining the hinge loss  in this we calculate the loss. \n",
        "  def hingeLoss_for_otimization(self, W, b, X, Y):\n",
        "    loss = 0.0\n",
        "    loss += .5*np.dot(W, W.T)\n",
        "    m = X.shape[0]\n",
        "    for i in range(m):\n",
        "      ti = Y[i] * (np.dot(W, X[i].T) + b)\n",
        "      loss += self.C *max(0, (1-ti))\n",
        "    return loss[0][0]\n",
        "\n",
        "  # Defining the fit function to optimise the W using stochastic gradient descent by using batch_size = 500 and learning_rate=0.001 and maximum number of iterations maxItr=500.\n",
        "  def fit_model(self, X, Y, batch_size=5000, learning_rate=0.001, maxItr=500):\n",
        "    no_of_features = X.shape[1]\n",
        "    no_of_samples = X.shape[0]\n",
        "    n = learning_rate\n",
        "    c = self.C\n",
        "  #Initialize the model parameters\n",
        "    W = np.zeros((1, no_of_features))\n",
        "    bias = 0\n",
        "  #Initial Loss\n",
        "    losses = []\n",
        "    for i in range(maxItr):\n",
        "  #Training Loop\n",
        "      l = self.hingeLoss_for_otimization(W, bias, X, Y)\n",
        "      losses. append(l)\n",
        "      ids = np.arange(no_of_samples)\n",
        "      np.random.shuffle(ids)\n",
        "  #Batch Gradient Descent with random shuffling of data samples\n",
        "      for batch_start in range(0, no_of_samples, batch_size):\n",
        "  #Assume 0 gradients for the batch\n",
        "        gradw = 0\n",
        "        gradb = 0\n",
        "  #Iterate over all examples in the mini batch to calculate the sub gradients\n",
        "        for j in range(batch_start, batch_start + batch_size):\n",
        "          if j < no_of_samples: \n",
        "            i = ids[j] \n",
        "            ti = Y[i] * (np.dot(W, X[i].T) + bias)\n",
        "            if ti > 1:\n",
        "              gradw += 0\n",
        "              gradb += 0\n",
        "            else:\n",
        "            # Updating the gradients.  \n",
        "              gradw += c * Y[i] * X[i]\n",
        "              gradb += c * Y[i]\n",
        "  # Updating Gradient for the batch Update W, B\n",
        "        W = W - n*W + n*gradw\n",
        "        bias = bias + n*gradb\n",
        "    self.W = W\n",
        "    self.b = bias\n",
        "    return W, bias, losses\n",
        "  # Defining the predict function\n",
        "  # Here yx calculates the X*W^T+b\n",
        "  def yx_input(self, X):\n",
        "      return np.dot(X,self.W.T) + self.b\n",
        "  def predict(self, X):\n",
        "      return np.where(self.yx_input(X) >= 0.0, 1, -1)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJCY02A8nK0F"
      },
      "source": [
        "## Step 6: Calculating optimized value of w using perceptron rule that is w[i+1] = w[i] + yi*xi by deifning the above declared model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWaOI7nyEKOb",
        "outputId": "3dce432c-ac20-4e82-9a3e-6de7ea1e17e4"
      },
      "source": [
        "model = mySVM(C=1000)\n",
        "W, b, losses = model.fit_model(X_train, y_train, maxItr=100)\n",
        "print(W,b,losses)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-9.18609815e+04 -8.79769884e+04  4.86842921e+05 -1.26640482e+05\n",
            "  -1.20324721e+01]] [-4961.] [2398000.0, 56615047296166.19, 522993411903575.56, 113985645904601.1, 294657549957694.6, 166959673911872.88, 110395921306296.47, 33536538619536.453, 29611791694958.336, 25705217109762.55, 21855944372223.926, 18943477416750.12, 16984949634536.803, 15030336944754.447, 13540705012901.676, 14453241878803.15, 13870560892417.182, 16500676204008.826, 14547031568274.367, 12916108233352.818, 13363365623913.838, 14721172093732.654, 17475807515950.55, 15520213586582.576, 13568528884873.725, 12637339266774.74, 15142057799869.678, 13191129026301.105, 12403242956284.035, 15069421703165.744, 13118638124356.879, 11647605833577.43, 13099864622985.285, 11327418357385.713, 11448492759203.982, 13591709301048.623, 11645901287923.066, 12952964240802.12, 15886277096028.648, 13933860610623.793, 11985347001167.576, 10622386948728.312, 12584377552729.623, 10644975182055.16, 13222869033672.094, 15675277362759.842, 12968431506370.11, 11021847776931.672, 10413063526755.89, 13621872154422.248, 11673982193482.178, 9737054804061.855, 12873026172281.287, 15332503709456.193, 12245146348600.945, 10300008452183.895, 9436168484000.451, 12544060988131.865, 10598325557128.5, 8687991987842.875, 10844414977423.416, 14700015037590.63, 11437546057505.232, 9494022539869.184, 8690754972769.988, 12110744716534.53, 9990650006499.256, 8056544818370.825, 11948456210011.584, 14732341511955.38, 11105151072031.164, 9162291998441.848, 7322930192306.535, 7995653896312.561, 12226409784076.355, 9134302636617.691, 7200655980694.729, 11313758191407.248, 14407658292753.44, 10548148796436.258, 7879370149837.444, 7263502514697.406, 12056143017663.598, 8688082929535.055, 6750055548871.739, 9808304998828.648, 13943389229094.193, 10072013196459.336, 7104710841530.014, 6862556147637.669, 12419047602641.062, 8607786878684.8, 6493361220381.951, 7490808285634.488, 13197415799266.283, 9323055972820.684, 6263705181662.378, 6821915883523.903, 12745808162435.223, 8870903929621.654]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDUC3aBjnZDO"
      },
      "source": [
        "## Step 7: Creating target Array and calculating the target values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjmcLnrGVuMd",
        "outputId": "7051544c-d15c-4851-dd7d-c9e63eb05a98"
      },
      "source": [
        "target = model.predict(X_test)\n",
        "t = model.predict(X_train)\n",
        "print(target,t,y_test)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]] [[-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " ...\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]] [[ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]\n",
            " [-1]\n",
            " [-1]\n",
            " [-1]\n",
            " [ 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFR8-UtNm8SP"
      },
      "source": [
        "## Step 8: Checking for Accuracy. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs7ja6NHm23v",
        "outputId": "c148c809-b159-4872-b04b-d4848af730b6"
      },
      "source": [
        "v1 = 0  \n",
        "v2 = 0\n",
        "c = 0\n",
        "i=0    \n",
        "for c in range( np.size( target ) ) :  \n",
        "  if y_test[c] == target[c] :            \n",
        "    v1 = v1 + 1\n",
        "for i in range( np.size( t ) ) :\n",
        "  if y_train[i] == t[i] :            \n",
        "    v2 = v2 + 1\n",
        "  i = i + 1\n",
        "print(\"Accuracy of Defined Model on test data :\",(v1/c)*100)     \n",
        "print(\"Accuracy of Defined Model on train data:\",(v2/i)*100)   "
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Defined Model on test data : 97.36842105263158\n",
            "Accuracy of Defined Model on train data: 97.99833194328608\n"
          ]
        }
      ]
    }
  ]
}